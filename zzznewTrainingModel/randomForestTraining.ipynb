{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YW8EHdQndE6Y"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# Imports for Voting Ensemble Training\n",
        "# ===============================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import re\n",
        "import math\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "\n",
        "# Machine learning\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(r\"D:\\QRusaderTrainedModel\\zzznewTrainingModel\\csvFiles\\merged_url_datasets.csv\")\n",
        "df.rename(columns={\"Label\":\"label\"}, inplace=True)\n"
      ],
      "metadata": {
        "id": "VDQwK5Eudboj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "8HVOiZaOddCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "UrgooLKodepK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail(10)"
      ],
      "metadata": {
        "id": "Pki-Cd78dgS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = df.label.value_counts()\n",
        "count"
      ],
      "metadata": {
        "id": "Ami0wb7KdjiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Optimized Parallel Feature Extraction (Refined Features)\n",
        "# ===============================\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from urllib.parse import urlparse, urlunparse\n",
        "import shelve\n",
        "import math\n",
        "import re\n",
        "\n",
        "# -------------------------------\n",
        "# Load dataset\n",
        "# -------------------------------\n",
        "df = pd.read_csv(r\"D:\\QRusaderTrainedModel\\zzznewTrainingModel\\csvFiles\\merged_url_datasets.csv\")\n",
        "df.rename(columns={\"Label\": \"label\", \"URL\": \"url\"}, inplace=True)\n",
        "\n",
        "# -------------------------------\n",
        "# Helper functions\n",
        "# -------------------------------\n",
        "def normalize_url(url: str) -> str:\n",
        "    if not url.startswith((\"http://\", \"https://\")):\n",
        "        url = \"http://\" + url\n",
        "    parsed = urlparse(url)\n",
        "    domain = parsed.netloc.lower()\n",
        "    if domain.startswith(\"www.\"):\n",
        "        domain = domain[4:]\n",
        "    path = parsed.path or \"/\"\n",
        "    return urlunparse((parsed.scheme.lower(), domain, path, parsed.params, parsed.query, parsed.fragment))\n",
        "\n",
        "def shannon_entropy(data):\n",
        "    if not data:\n",
        "        return 0\n",
        "    prob = [float(data.count(c))/len(data) for c in set(data)]\n",
        "    return -sum(p*math.log2(p) for p in prob)\n",
        "\n",
        "shortening_services = [\"bit.ly\",\"tinyurl\",\"goo.gl\",\"t.co\",\"ow.ly\",\"shorte.st\",\"cutt.ly\"]\n",
        "suspicious_keywords = [\"secure\",\"account\",\"login\",\"update\",\"free\",\"bonus\",\"ebayisapi\",\n",
        "                       \"banking\",\"confirm\",\"signin\",\"verification\"]\n",
        "\n",
        "def is_shortened(url): return int(any(s in url for s in shortening_services))\n",
        "def has_ip(url): return int(bool(re.match(r\"http[s]?://\\d+\\.\\d+\\.\\d+\\.\\d+\", url)))\n",
        "def contains_suspicious_word(url): return sum(word in url.lower() for word in suspicious_keywords)\n",
        "\n",
        "# -------------------------------\n",
        "# WHOIS safe wrapper with caching\n",
        "# -------------------------------\n",
        "CACHE_FILE = \"whois_cache.db\"\n",
        "def get_whois_safe(domain):\n",
        "    with shelve.open(CACHE_FILE) as cache:\n",
        "        if domain in cache:\n",
        "            return cache[domain][\"has_whois\"], cache[domain][\"domain_age_days\"]\n",
        "        try:\n",
        "            import whois\n",
        "            w = whois.whois(domain)\n",
        "            if not hasattr(w, \"creation_date\") or not w.creation_date:\n",
        "                has_dns, age_days = 0, 0\n",
        "            else:\n",
        "                has_dns = 1\n",
        "                creation_date = w.creation_date[0] if isinstance(w.creation_date, list) else w.creation_date\n",
        "                age_days = (pd.Timestamp.now() - pd.Timestamp(creation_date)).days\n",
        "        except Exception:\n",
        "            has_dns, age_days = 0, 0\n",
        "        cache[domain] = {\"has_whois\": has_dns, \"domain_age_days\": age_days}\n",
        "        return has_dns, age_days\n",
        "\n",
        "# -------------------------------\n",
        "# Feature extraction\n",
        "# -------------------------------\n",
        "def extract_features(url):\n",
        "    url_norm = normalize_url(url)\n",
        "    parsed = urlparse(url_norm)\n",
        "    domain = parsed.netloc\n",
        "    path = parsed.path or \"/\"\n",
        "\n",
        "    # WHOIS features\n",
        "    has_whois, domain_age_days = get_whois_safe(domain)\n",
        "\n",
        "    # URL structure\n",
        "    total_special_char = sum(url_norm.count(c) for c in ['@','?','-','=','.','!','#','$','&','~','*','%','+','^','_'])\n",
        "    path_tokens = [t for t in path.split('/') if t]\n",
        "\n",
        "    # Entropy\n",
        "    domain_entropy = shannon_entropy(domain)\n",
        "    path_entropy = shannon_entropy(path)\n",
        "\n",
        "    # N-grams\n",
        "    bigrams = [\"_\".join(path_tokens[i:i+2]) for i in range(len(path_tokens)-1)]\n",
        "    trigrams = [\"_\".join(path_tokens[i:i+3]) for i in range(len(path_tokens)-2)]\n",
        "\n",
        "    return {\n",
        "        \"url_original\": url,\n",
        "        \"normalized_url\": url_norm,\n",
        "        \"url_length\": len(url_norm),\n",
        "        \"Shortining_Service\": is_shortened(url_norm),\n",
        "        \"having_ip_address\": has_ip(url_norm),\n",
        "        \"subdomain_count\": max(domain.count(\".\")-1,0),\n",
        "        \"subdomain_ratio\": max(domain.count(\".\")-1,0)/max(1,len(domain)),\n",
        "        \"path_depth\": path.count('/'),\n",
        "        \"path_length\": len(path),\n",
        "        \"param_count\": parsed.query.count(\"=\"),\n",
        "        \"digit_letter_ratio\": sum(c.isdigit() for c in url_norm)/max(1,sum(c.isalpha() for c in url_norm)),\n",
        "        \"domain_entropy\": domain_entropy,\n",
        "        \"path_entropy\": path_entropy,\n",
        "        \"total_special_char\": total_special_char,\n",
        "        \"special_char_ratio\": total_special_char/max(1,len(url_norm)),\n",
        "        \"risky_tld\": int(domain.split('.')[-1] in [\"zip\",\"xyz\",\"top\",\"club\",\"info\"]),\n",
        "        \"tld_length\": len(domain.split('.')[-1]),\n",
        "        \"suspicious_word_count\": contains_suspicious_word(url_norm),\n",
        "        \"url_upper_ratio\": sum(1 for c in url_norm if c.isupper())/max(1,len(url_norm)),\n",
        "        \"repeated_char_count\": sum(url_norm.count(c*2) for c in set(url_norm)),\n",
        "        \"path_token_count\": len(path_tokens),\n",
        "        \"unique_bigrams\": len(set(bigrams)),\n",
        "        \"unique_trigrams\": len(set(trigrams)),\n",
        "        \"has_whois\": has_whois,\n",
        "        \"domain_age_days\": domain_age_days\n",
        "    }\n"
      ],
      "metadata": {
        "id": "UWXvmg_Ldln0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 4️⃣ Parallel Feature Extraction (Optimized with WHOIS caching & refined features)\n",
        "# ===============================\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from urllib.parse import urlparse\n",
        "import shelve\n",
        "\n",
        "# -------------------------------\n",
        "# Load dataset\n",
        "# -------------------------------\n",
        "df = pd.read_csv(r\"D:\\QRusaderTrainedModel\\zzznewTrainingModel\\csvFiles\\merged_url_datasets.csv\")\n",
        "df.rename(columns={\"Label\": \"label\", \"URL\": \"url\"}, inplace=True)\n",
        "\n",
        "# -------------------------------\n",
        "# Pre-cache WHOIS results (so each domain is queried once)\n",
        "# -------------------------------\n",
        "domains = df[\"url\"].apply(lambda u: urlparse(normalize_url(u)).netloc).unique()\n",
        "\n",
        "print(f\"🔍 Pre-caching WHOIS for {len(domains)} unique domains...\")\n",
        "with shelve.open(\"whois_cache.db\") as cache:\n",
        "    for i, domain in enumerate(domains, 1):\n",
        "        if domain not in cache:  # only fetch new ones\n",
        "            has_whois, age_days = get_whois_safe(domain)\n",
        "            cache[domain] = {\"has_whois\": has_whois, \"domain_age_days\": age_days}\n",
        "        if i % 100 == 0 or i == len(domains):\n",
        "            print(f\"Cached {i}/{len(domains)} domains\")\n",
        "\n",
        "# -------------------------------\n",
        "# Safe wrapper for feature extraction\n",
        "# -------------------------------\n",
        "def safe_extract(url):\n",
        "    try:\n",
        "        feats = extract_features(url)\n",
        "        # Ensure numeric fields are filled\n",
        "        for key in [\"has_whois\", \"domain_age_days\", \"unique_bigrams\", \"unique_trigrams\"]:\n",
        "            if feats.get(key) is None:\n",
        "                feats[key] = 0\n",
        "        return feats\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing URL {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# -------------------------------\n",
        "# Parallel feature extraction\n",
        "# -------------------------------\n",
        "features = []\n",
        "with ThreadPoolExecutor(max_workers=20) as executor:\n",
        "    future_to_url = {executor.submit(safe_extract, url): url for url in df[\"url\"]}\n",
        "\n",
        "    for i, future in enumerate(as_completed(future_to_url), 1):\n",
        "        result = future.result()\n",
        "        if result is not None:\n",
        "            features.append(result)\n",
        "        if i % 500 == 0 or i == len(df):\n",
        "            print(f\"✅ Processed {i}/{len(df)} URLs\")\n",
        "\n",
        "# -------------------------------\n",
        "# Convert to DataFrame\n",
        "# -------------------------------\n",
        "features_df = pd.DataFrame(features)\n",
        "print(f\"✅ Feature extraction completed. Shape: {features_df.shape}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Merge labels\n",
        "# -------------------------------\n",
        "features_df = features_df.merge(\n",
        "    df[['url','label']], left_on='url_original', right_on='url', how='left'\n",
        ")\n",
        "features_df.drop(columns=['url'], inplace=True)\n",
        "\n",
        "# -------------------------------\n",
        "# Ensure numeric columns are numeric\n",
        "# -------------------------------\n",
        "numeric_cols = [\n",
        "    \"url_length\", \"Shortining_Service\", \"having_ip_address\",\n",
        "    \"subdomain_count\", \"subdomain_ratio\", \"path_depth\", \"path_length\",\n",
        "    \"param_count\", \"digit_letter_ratio\",\n",
        "    \"domain_entropy\", \"path_entropy\",\n",
        "    \"total_special_char\", \"special_char_ratio\",\n",
        "    \"risky_tld\", \"tld_length\", \"suspicious_word_count\",\n",
        "    \"url_upper_ratio\", \"repeated_char_count\", \"path_token_count\",\n",
        "    \"unique_bigrams\", \"unique_trigrams\",\n",
        "    \"has_whois\", \"domain_age_days\"\n",
        "]\n",
        "\n",
        "features_df[numeric_cols] = features_df[numeric_cols].apply(\n",
        "    pd.to_numeric, errors='coerce'\n",
        ").fillna(0)\n",
        "\n",
        "# -------------------------------\n",
        "# Save to CSV (fast)\n",
        "# -------------------------------\n",
        "output_file = Path(r\"D:\\QRusaderTrainedModel\\zzznewTrainingModel\\csvFiles\\url_features_detailed.csv\")\n",
        "output_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "features_df.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
        "print(f\"✅ Features with labels and refined enhancements saved at: {output_file}\")\n"
      ],
      "metadata": {
        "id": "IjIvA9iGdnig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count how many have WHOIS\n",
        "total_urls = len(features_df)\n",
        "with_whois = features_df['has_whois'].sum()  # sum of 1's\n",
        "without_whois = total_urls - with_whois\n",
        "\n",
        "print(f\"Total URLs: {total_urls}\")\n",
        "print(f\"With WHOIS: {with_whois} ({with_whois/total_urls*100:.2f}%)\")\n",
        "print(f\"Without WHOIS: {without_whois} ({without_whois/total_urls*100:.2f}%)\")"
      ],
      "metadata": {
        "id": "ghdP4csGdo3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 5️⃣ Verify specific URL presence\n",
        "# ===============================\n",
        "\n",
        "url_to_check = \"https://www.google.com\"\n",
        "normalized_url = normalize_url(url_to_check)\n",
        "\n",
        "# Search in the features_df\n",
        "google_row = features_df[features_df['normalized_url'] == normalized_url]\n",
        "# Show all columns for this row\n",
        "pd.set_option('display.max_columns', None)  # show all columns\n",
        "pd.set_option('display.width', 200)         # set display width to avoid wrapping\n",
        "if not google_row.empty:\n",
        "    print(\"✅ Found URL in dataset:\")\n",
        "    print(google_row)\n",
        "else:\n",
        "    print(\"❌ URL not found in dataset.\")\n"
      ],
      "metadata": {
        "id": "YmauNK4pdqQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Step 5: Exploratory Data Analysis (Reduced + Normalized Features)\n",
        "# ===============================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "\n",
        "# Ensure features_df exists\n",
        "if 'features_df' not in globals():\n",
        "    raise ValueError(\"features_df not defined. Run extract_features on your dataset first.\")\n",
        "\n",
        "# -------------------------------\n",
        "# 1️⃣ Numeric Features - Histograms\n",
        "# -------------------------------\n",
        "numeric_features = features_df.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# Exclude non-informative numeric columns\n",
        "exclude_numeric = ['has_whois']  # you can keep domain_age_days for analysis\n",
        "numeric_features = numeric_features.drop(columns=[col for col in exclude_numeric if col in numeric_features.columns], errors='ignore')\n",
        "\n",
        "num_cols = len(numeric_features.columns)\n",
        "num_rows = math.ceil(num_cols / 3)  # 3 plots per row\n",
        "\n",
        "fig, axes = plt.subplots(num_rows, 3, figsize=(18, num_rows*4))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(numeric_features.columns):\n",
        "    sns.histplot(numeric_features[col], bins=30, kde=False, ax=axes[i], color='skyblue')\n",
        "    axes[i].set_title(f\"Distribution of {col}\", fontsize=12)\n",
        "    axes[i].set_xlabel(col, fontsize=10)\n",
        "    axes[i].set_ylabel(\"Count\", fontsize=10)\n",
        "\n",
        "# Hide empty subplots\n",
        "for j in range(i+1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# 2️⃣ Binary / Categorical Features - Countplots\n",
        "# -------------------------------\n",
        "binary_features = [\n",
        "    \"Shortining_Service\",\n",
        "    \"having_ip_address\",\n",
        "    \"suspicious_word_count\",\n",
        "    \"risky_tld\",\n",
        "    \"has_whois\"\n",
        "]\n",
        "\n",
        "binary_features_existing = [col for col in binary_features if col in features_df.columns]\n",
        "\n",
        "for col in binary_features_existing:\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.countplot(x=col, data=features_df, palette='Set2')\n",
        "    plt.title(f\"Countplot of {col}\", fontsize=12)\n",
        "    plt.xlabel(col, fontsize=10)\n",
        "    plt.ylabel(\"Count\", fontsize=10)\n",
        "    plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# 3️⃣ Correlation Heatmap - Numeric Features Only\n",
        "# -------------------------------\n",
        "plt.figure(figsize=(18,16))\n",
        "sns.heatmap(numeric_features.corr(), annot=True, fmt=\".2f\", cmap=\"magma\", cbar=True)\n",
        "plt.title(\"Correlation Heatmap of Numeric Features\", fontsize=16)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "4dexo2YWdrg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Training & Evaluation: Random Forest (Refined Features)\n",
        "# ===============================\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, roc_curve, auc,\n",
        "    confusion_matrix, ConfusionMatrixDisplay\n",
        ")\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------------\n",
        "# 1️⃣ Load dataset\n",
        "# -------------------------------\n",
        "df = pd.read_csv(r\"D:\\QRusaderTrainedModel\\zzznewTrainingModel\\csvFiles\\url_features_detailed.csv\")\n",
        "df = df.dropna(subset=['label'])\n",
        "\n",
        "# -------------------------------\n",
        "# 2️⃣ Define numeric features\n",
        "# -------------------------------\n",
        "numeric_cols = [\n",
        "    \"url_length\", \"Shortining_Service\", \"having_ip_address\",\n",
        "    \"subdomain_count\", \"subdomain_ratio\", \"path_depth\", \"path_length\",\n",
        "    \"param_count\", \"digit_letter_ratio\",\n",
        "    \"domain_entropy\", \"path_entropy\",\n",
        "    \"total_special_char\", \"special_char_ratio\",\n",
        "    \"risky_tld\", \"tld_length\", \"suspicious_word_count\",\n",
        "    \"url_upper_ratio\", \"repeated_char_count\", \"path_token_count\",\n",
        "    \"unique_bigrams\", \"unique_trigrams\",\n",
        "    \"has_whois\", \"domain_age_days\"\n",
        "]\n",
        "\n",
        "X = df[numeric_cols].fillna(0)\n",
        "y = df['label']\n",
        "\n",
        "# Label encoding\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "y_bin = label_binarize(y_encoded, classes=range(len(le.classes_)))\n",
        "\n",
        "# -------------------------------\n",
        "# 3️⃣ Train/test split\n",
        "# -------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "y_test_bin = label_binarize(y_test, classes=range(len(le.classes_)))\n",
        "\n",
        "# -------------------------------\n",
        "# 4️⃣ Initialize Random Forest\n",
        "# -------------------------------\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=1500,\n",
        "    max_depth=None,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    max_features='log2',\n",
        "    random_state=42,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 5️⃣ Train & Evaluate\n",
        "# -------------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"### Training & Evaluating: Random Forest\")\n",
        "\n",
        "# Fit model\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc*100:.2f}%\")\n",
        "\n",
        "# Decode labels for reporting\n",
        "y_test_labels = le.inverse_transform(y_test)\n",
        "y_pred_labels = le.inverse_transform(y_pred)\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test_labels, y_pred_labels))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test_labels, y_pred_labels, labels=le.classes_)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=le.classes_)\n",
        "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
        "plt.title(f\"Confusion Matrix: Random Forest\")\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# 6️⃣ ROC Curve\n",
        "# -------------------------------\n",
        "y_score = rf.predict_proba(X_test)\n",
        "fpr, tpr, roc_auc = {}, {}, {}\n",
        "\n",
        "if len(le.classes_) == 2:\n",
        "    fpr[1], tpr[1], _ = roc_curve(y_test, y_score[:, 1])\n",
        "    roc_auc[1] = auc(fpr[1], tpr[1])\n",
        "    fpr[\"micro\"], tpr[\"micro\"], roc_auc[\"micro\"] = fpr[1], tpr[1], roc_auc[1]\n",
        "else:\n",
        "    for i in range(len(le.classes_)):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), y_score.ravel())\n",
        "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(len(le.classes_))]))\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "    for i in range(len(le.classes_)):\n",
        "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "    mean_tpr /= len(le.classes_)\n",
        "    fpr[\"macro\"], tpr[\"macro\"] = all_fpr, mean_tpr\n",
        "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"], label=f\"Random Forest (AUC={roc_auc['micro']:.2f})\")\n",
        "plt.plot([0,1],[0,1],'k--', label=\"Random\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve: Random Forest\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# 7️⃣ Feature Importance\n",
        "# -------------------------------\n",
        "feat_imp_df = pd.DataFrame({\n",
        "    \"feature\": X.columns,\n",
        "    \"importance\": rf.feature_importances_\n",
        "}).sort_values(by=\"importance\", ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=\"importance\", y=\"feature\", data=feat_imp_df.head(15), palette=\"viridis\")\n",
        "plt.title(\"Top 15 Feature Importances: Random Forest\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6jnMgaa4ds2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Step 6: Check 10 Random URLs\n",
        "# ===============================\n",
        "import random\n",
        "\n",
        "# Sample 10 random rows from the dataset\n",
        "sample_df = features_df.sample(n=10, random_state=50)  # fix random_state for reproducibility\n",
        "\n",
        "# Select relevant columns to display\n",
        "display_cols = ['url_original', 'label', 'predicted_label', 'predicted_prob_malicious']\n",
        "\n",
        "print(\"🔹 Random 10 URL Predictions:\")\n",
        "print(sample_df[display_cols].reset_index(drop=True))\n"
      ],
      "metadata": {
        "id": "s2LgLwszdwTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Save Random Forest Model & Metadata\n",
        "# ===============================\n",
        "\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Directory to save the model\n",
        "model_dir = r\"D:\\QRusaderTrainedModel\\zzznewTrainingModel\\saved_models\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# Save the Random Forest model\n",
        "rf_model_path = os.path.join(model_dir, \"random_forest_model.pkl\")\n",
        "joblib.dump(rf, rf_model_path)\n",
        "\n",
        "# Save the LabelEncoder\n",
        "le_path = os.path.join(model_dir, \"label_encoder.pkl\")\n",
        "joblib.dump(le, le_path)\n",
        "\n",
        "# Save the feature columns list (numeric columns used in training)\n",
        "features_path = os.path.join(model_dir, \"feature_columns.pkl\")\n",
        "joblib.dump(numeric_cols, features_path)\n",
        "\n",
        "print(f\"✅ Random Forest model, encoder, and feature columns saved to {model_dir}\")\n"
      ],
      "metadata": {
        "id": "QpeQrk2Sdya5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Predict Safe URLs Using Saved Random Forest\n",
        "# ===============================\n",
        "\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from urllib.parse import urlparse, urlunparse\n",
        "import math\n",
        "import re\n",
        "\n",
        "# -------------------------------\n",
        "# 1️⃣ Load saved model & metadata\n",
        "# -------------------------------\n",
        "model_dir = r\"D:\\QRusaderTrainedModel\\zzznewTrainingModel\\saved_models\"\n",
        "\n",
        "rf = joblib.load(f\"{model_dir}/random_forest_model.pkl\")\n",
        "le = joblib.load(f\"{model_dir}/label_encoder.pkl\")\n",
        "feature_columns = joblib.load(f\"{model_dir}/feature_columns.pkl\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2️⃣ Helper functions (same as in feature extraction)\n",
        "# -------------------------------\n",
        "def normalize_url(url: str) -> str:\n",
        "    if not url.startswith((\"http://\", \"https://\")):\n",
        "        url = \"http://\" + url\n",
        "    parsed = urlparse(url)\n",
        "    domain = parsed.netloc.lower()\n",
        "    if domain.startswith(\"www.\"):\n",
        "        domain = domain[4:]\n",
        "    path = parsed.path or \"/\"\n",
        "    return urlunparse((parsed.scheme.lower(), domain, path, parsed.params, parsed.query, parsed.fragment))\n",
        "\n",
        "def shannon_entropy(data):\n",
        "    if not data:\n",
        "        return 0\n",
        "    prob = [float(data.count(c))/len(data) for c in set(data)]\n",
        "    return -sum(p*math.log2(p) for p in prob)\n",
        "\n",
        "shortening_services = [\"bit.ly\",\"tinyurl\",\"goo.gl\",\"t.co\",\"ow.ly\",\"shorte.st\",\"cutt.ly\"]\n",
        "suspicious_keywords = [\"secure\",\"account\",\"login\",\"update\",\"free\",\"bonus\",\"ebayisapi\",\n",
        "                       \"banking\",\"confirm\",\"signin\",\"verification\"]\n",
        "\n",
        "def is_shortened(url): return int(any(s in url for s in shortening_services))\n",
        "def has_ip(url): return int(bool(re.match(r\"http[s]?://\\d+\\.\\d+\\.\\d+\\.\\d+\", url)))\n",
        "def contains_suspicious_word(url): return sum(word in url.lower() for word in suspicious_keywords)\n",
        "\n",
        "# WHOIS caching function (optional if available)\n",
        "CACHE_FILE = \"whois_cache.db\"\n",
        "def get_whois_safe(domain):\n",
        "    import shelve\n",
        "    with shelve.open(CACHE_FILE) as cache:\n",
        "        if domain in cache:\n",
        "            return cache[domain][\"has_whois\"], cache[domain][\"domain_age_days\"]\n",
        "        return 0, 0  # default if no WHOIS info\n",
        "\n",
        "# -------------------------------\n",
        "# 3️⃣ Feature extraction for prediction\n",
        "# -------------------------------\n",
        "def extract_features_for_prediction(url):\n",
        "    url_norm = normalize_url(url)\n",
        "    parsed = urlparse(url_norm)\n",
        "    domain = parsed.netloc\n",
        "    path = parsed.path or \"/\"\n",
        "\n",
        "    has_whois, domain_age_days = get_whois_safe(domain)\n",
        "    total_special_char = sum(url_norm.count(c) for c in ['@','?','-','=','.','!','#','$','&','~','*','%','+','^','_'])\n",
        "    path_tokens = [t for t in path.split('/') if t]\n",
        "\n",
        "    return {\n",
        "        \"url_length\": len(url_norm),\n",
        "        \"Shortining_Service\": is_shortened(url_norm),\n",
        "        \"having_ip_address\": has_ip(url_norm),\n",
        "        \"subdomain_count\": max(domain.count(\".\")-1,0),\n",
        "        \"subdomain_ratio\": max(domain.count(\".\")-1,0)/max(1,len(domain)),\n",
        "        \"path_depth\": path.count('/'),\n",
        "        \"path_length\": len(path),\n",
        "        \"param_count\": parsed.query.count(\"=\"),\n",
        "        \"digit_letter_ratio\": sum(c.isdigit() for c in url_norm)/max(1,sum(c.isalpha() for c in url_norm)),\n",
        "        \"domain_entropy\": shannon_entropy(domain),\n",
        "        \"path_entropy\": shannon_entropy(path),\n",
        "        \"total_special_char\": total_special_char,\n",
        "        \"special_char_ratio\": total_special_char/max(1,len(url_norm)),\n",
        "        \"risky_tld\": int(domain.split('.')[-1] in [\"zip\",\"xyz\",\"top\",\"club\",\"info\"]),\n",
        "        \"tld_length\": len(domain.split('.')[-1]),\n",
        "        \"suspicious_word_count\": contains_suspicious_word(url_norm),\n",
        "        \"url_upper_ratio\": sum(1 for c in url_norm if c.isupper())/max(1,len(url_norm)),\n",
        "        \"repeated_char_count\": sum(url_norm.count(c*2) for c in set(url_norm)),\n",
        "        \"path_token_count\": len(path_tokens),\n",
        "        \"unique_bigrams\": len(set([\"_\".join(path_tokens[i:i+2]) for i in range(len(path_tokens)-1)])),\n",
        "        \"unique_trigrams\": len(set([\"_\".join(path_tokens[i:i+3]) for i in range(len(path_tokens)-2)])),\n",
        "        \"has_whois\": has_whois,\n",
        "        \"domain_age_days\": domain_age_days\n",
        "    }\n",
        "\n",
        "# -------------------------------\n",
        "# 4️⃣ Predict a list of URLs\n",
        "# -------------------------------\n",
        "urls_to_check = [\n",
        "    \"https://drive.google.com/drive/u/1/my-drive\"\n",
        "]\n",
        "\n",
        "\n",
        "# Extract features\n",
        "features_list = [extract_features_for_prediction(u) for u in urls_to_check]\n",
        "X_pred = pd.DataFrame(features_list)[feature_columns].fillna(0)\n",
        "\n",
        "# Predict labels and probabilities\n",
        "y_pred_encoded = rf.predict(X_pred)\n",
        "y_pred_labels = le.inverse_transform(y_pred_encoded)\n",
        "y_pred_probs = rf.predict_proba(X_pred)\n",
        "\n",
        "# Prepare results\n",
        "results_df = pd.DataFrame({\n",
        "    \"url\": urls_to_check,\n",
        "    \"predicted_label\": y_pred_labels,\n",
        "    \"prob_safe\": y_pred_probs[:, le.transform(['safe'])[0]],\n",
        "    \"prob_malicious\": y_pred_probs[:, le.transform(['malicious'])[0]]\n",
        "})\n",
        "\n",
        "def risk_level(prob_malicious):\n",
        "    if prob_malicious < 0.4:\n",
        "        return \"Safe\"\n",
        "    elif prob_malicious <= 0.7:\n",
        "        return \"Medium\"\n",
        "    else:\n",
        "        return \"High\"\n",
        "\n",
        "results_df['risk_level'] = results_df['prob_malicious'].apply(risk_level)\n",
        "\n",
        "# -------------------------------\n",
        "# 6️⃣ Display Results\n",
        "# -------------------------------\n",
        "print(\"🔹 Predictions for 10 URLs with Risk Levels:\")\n",
        "print(results_df[['url', 'predicted_label', 'prob_safe', 'prob_malicious', 'risk_level']])\n"
      ],
      "metadata": {
        "id": "jhfsAEordzmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Batch Predict All URLs and Save CSV (Essential Columns Only)\n",
        "# ===============================\n",
        "\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from urllib.parse import urlparse, urlunparse\n",
        "import math\n",
        "import re\n",
        "\n",
        "# -------------------------------\n",
        "# 1️⃣ Load saved Random Forest model & metadata\n",
        "# -------------------------------\n",
        "model_dir = r\"D:\\QRusaderTrainedModel\\zzznewTrainingModel\\saved_models\"\n",
        "\n",
        "rf = joblib.load(f\"{model_dir}/random_forest_model.pkl\")\n",
        "le = joblib.load(f\"{model_dir}/label_encoder.pkl\")\n",
        "feature_columns = joblib.load(f\"{model_dir}/feature_columns.pkl\")\n",
        "\n",
        "# Load your full dataset\n",
        "features_df = pd.read_csv(r\"D:\\QRusaderTrainedModel\\zzznewTrainingModel\\csvFiles\\url_features_detailed.csv\")\n",
        "features_df = features_df.dropna(subset=['label'])\n",
        "\n",
        "# -------------------------------\n",
        "# 2️⃣ Helper functions\n",
        "# -------------------------------\n",
        "def normalize_url(url: str) -> str:\n",
        "    if not url.startswith((\"http://\", \"https://\")):\n",
        "        url = \"http://\" + url\n",
        "    parsed = urlparse(url)\n",
        "    domain = parsed.netloc.lower()\n",
        "    if domain.startswith(\"www.\"):\n",
        "        domain = domain[4:]\n",
        "    path = parsed.path or \"/\"\n",
        "    return urlunparse((parsed.scheme.lower(), domain, path, parsed.params, parsed.query, parsed.fragment))\n",
        "\n",
        "# WHOIS caching\n",
        "CACHE_FILE = \"whois_cache.db\"\n",
        "def get_whois_safe(domain):\n",
        "    import shelve\n",
        "    with shelve.open(CACHE_FILE) as cache:\n",
        "        if domain in cache:\n",
        "            return cache[domain][\"has_whois\"], cache[domain][\"domain_age_days\"]\n",
        "        return 0, 0\n",
        "\n",
        "# Other helpers same as before (entropy, shortened, IP, suspicious words)\n",
        "def shannon_entropy(data):\n",
        "    if not data: return 0\n",
        "    prob = [float(data.count(c))/len(data) for c in set(data)]\n",
        "    return -sum(p*math.log2(p) for p in prob)\n",
        "\n",
        "shortening_services = [\"bit.ly\",\"tinyurl\",\"goo.gl\",\"t.co\",\"ow.ly\",\"shorte.st\",\"cutt.ly\"]\n",
        "suspicious_keywords = [\"secure\",\"account\",\"login\",\"update\",\"free\",\"bonus\",\"ebayisapi\",\n",
        "                       \"banking\",\"confirm\",\"signin\",\"verification\"]\n",
        "def is_shortened(url): return int(any(s in url for s in shortening_services))\n",
        "def has_ip(url): return int(bool(re.match(r\"http[s]?://\\d+\\.\\d+\\.\\d+\\.\\d+\", url)))\n",
        "def contains_suspicious_word(url): return sum(word in url.lower() for word in suspicious_keywords)\n",
        "\n",
        "# -------------------------------\n",
        "# 3️⃣ Feature extraction function\n",
        "# -------------------------------\n",
        "def extract_features_for_prediction(url):\n",
        "    url_norm = normalize_url(url)\n",
        "    parsed = urlparse(url_norm)\n",
        "    domain = parsed.netloc\n",
        "    path = parsed.path or \"/\"\n",
        "    has_whois, domain_age_days = get_whois_safe(domain)\n",
        "    total_special_char = sum(url_norm.count(c) for c in ['@','?','-','=','.','!','#','$','&','~','*','%','+','^','_'])\n",
        "    path_tokens = [t for t in path.split('/') if t]\n",
        "    return {\n",
        "        \"url_length\": len(url_norm),\n",
        "        \"Shortining_Service\": is_shortened(url_norm),\n",
        "        \"having_ip_address\": has_ip(url_norm),\n",
        "        \"subdomain_count\": max(domain.count(\".\")-1,0),\n",
        "        \"subdomain_ratio\": max(domain.count(\".\")-1,0)/max(1,len(domain)),\n",
        "        \"path_depth\": path.count('/'),\n",
        "        \"path_length\": len(path),\n",
        "        \"param_count\": parsed.query.count(\"=\"),\n",
        "        \"digit_letter_ratio\": sum(c.isdigit() for c in url_norm)/max(1,sum(c.isalpha() for c in url_norm)),\n",
        "        \"domain_entropy\": shannon_entropy(domain),\n",
        "        \"path_entropy\": shannon_entropy(path),\n",
        "        \"total_special_char\": total_special_char,\n",
        "        \"special_char_ratio\": total_special_char/max(1,len(url_norm)),\n",
        "        \"risky_tld\": int(domain.split('.')[-1] in [\"zip\",\"xyz\",\"top\",\"club\",\"info\"]),\n",
        "        \"tld_length\": len(domain.split('.')[-1]),\n",
        "        \"suspicious_word_count\": contains_suspicious_word(url_norm),\n",
        "        \"url_upper_ratio\": sum(1 for c in url_norm if c.isupper())/max(1,len(url_norm)),\n",
        "        \"repeated_char_count\": sum(url_norm.count(c*2) for c in set(url_norm)),\n",
        "        \"path_token_count\": len(path_tokens),\n",
        "        \"unique_bigrams\": len(set([\"_\".join(path_tokens[i:i+2]) for i in range(len(path_tokens)-1)])),\n",
        "        \"unique_trigrams\": len(set([\"_\".join(path_tokens[i:i+3]) for i in range(len(path_tokens)-2)])),\n",
        "        \"has_whois\": has_whois,\n",
        "        \"domain_age_days\": domain_age_days\n",
        "    }\n",
        "\n",
        "# -------------------------------\n",
        "# 4️⃣ Extract features & predict\n",
        "# -------------------------------\n",
        "features_list = [extract_features_for_prediction(u) for u in features_df['url_original']]\n",
        "X_pred = pd.DataFrame(features_list)[feature_columns].fillna(0)\n",
        "\n",
        "y_pred_encoded = rf.predict(X_pred)\n",
        "y_pred_labels = le.inverse_transform(y_pred_encoded)\n",
        "y_pred_probs = rf.predict_proba(X_pred)\n",
        "\n",
        "# -------------------------------\n",
        "# 5️⃣ Prepare output DataFrame with essential columns\n",
        "# -------------------------------\n",
        "output_df = pd.DataFrame({\n",
        "    \"url_original\": features_df['url_original'],\n",
        "    \"normalized_url\": features_df['normalized_url'],\n",
        "    \"label\": features_df['label'],\n",
        "    \"predicted_label\": y_pred_labels,\n",
        "    \"prob_safe\": y_pred_probs[:, le.transform(['safe'])[0]],\n",
        "    \"prob_malicious\": y_pred_probs[:, le.transform(['malicious'])[0]]\n",
        "})\n",
        "\n",
        "# Apply risk levels\n",
        "def get_risk_level(prob_malicious):\n",
        "    if prob_malicious <= 0.4: return \"Safe\"\n",
        "    elif prob_malicious <= 0.7: return \"Medium\"\n",
        "    else: return \"High\"\n",
        "\n",
        "output_df['risk_level'] = output_df['prob_malicious'].apply(get_risk_level)\n",
        "\n",
        "# -------------------------------\n",
        "# 6️⃣ Save new CSV\n",
        "# -------------------------------\n",
        "output_file = r\"D:\\QRusaderTrainedModel\\zzznewTrainingModel\\csvFiles\\predicted_urls.csv\"\n",
        "output_df.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
        "print(f\"✅ Predictions saved at: {output_file}\")\n"
      ],
      "metadata": {
        "id": "pgF9XDtbd11h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}